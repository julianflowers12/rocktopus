% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/txt_to_parquet.R
\name{txt_to_parquet_dataset}
\alias{txt_to_parquet_dataset}
\title{Convert a large text file (CSV/TSV) to a Parquet dataset with optional partitioning.
This function uses DuckDB for efficient chunked reading and writing.
 @param in_file Path to the input text file (CSV/TSV).
 @param out_dir Directory to save the Parquet dataset (default: "par
 quet_dataset").
 @param chunk_size Number of rows per chunk (default: auto-calculated for
 ~200 MB chunks).
 @param dbdir Path to DuckDB database file (default: in-memory).
 @param partition_cols Character vector of column names to partition by (default: NULL).
 @param return_arrow Logical, whether to return an Arrow dataset object (default: FALSE).
 @param compression Compression algorithm for Parquet files (default: "ZSTD").
 @param repartition Optional integer to repartition the dataset after writing (default: NULL).
 @param log_perf Logical, whether to log performance metrics (default: TRUE).
 @param log_file Optional path to a log file to save performance metrics (default: NULL).
 @return A list containing:
 \item{summary}{A data frame summarizing the partitioning (if any).}
 \item{dataset}{(Optional) An Arrow dataset object if return_arrow is TRUE.}
 \item{perf}{A list of performance metrics including time taken, rows processed, input/output sizes, etc.}
 @examples
 \dontrun{
 result <- txt_to_parquet_dataset(
     in_file = "large_data.csv",
     out_dir = "parquet_data",
     chunk_size = 1e6,
     partition_cols = c("year", "month"),
     return_arrow = TRUE,
     compression = "ZSTD"
     )
 print(result$summary)
 print(result$perf)
 if (!is.null(result$dataset)) {
     print(result$dataset)
 }
 }
 @import DBI duckdb progressr glue jsonlite readr
 @importFrom arrow open_dataset
 @importFrom utils write.csv
 @export
 @details
 This function reads a large text file (CSV/TSV) in chunks using DuckDB,
 writes the data to Parquet files with optional partitioning, and provides
 performance metrics. It can also return an Arrow dataset object for further
 analysis in R.
 The function automatically calculates an optimal chunk size to balance
 memory usage and performance, targeting approximately 200 MB per chunk.
 If partitioning columns are specified, the output Parquet files will be organized
 into subdirectories based on the unique values of those columns.
 Compression can be specified to reduce the size of the output files, with "ZSTD"
 being the default for a good balance of speed and compression ratio.
 Performance metrics include total time taken, rows processed, input and output
 file sizes, and rows per second.
 If a log file path is provided, the performance metrics will be saved in JSON
 format for easy parsing and analysis.
 Note that this function requires the DuckDB and Arrow packages to be installed
 and may require additional system dependencies for optimal performance.
 Ensure that the input file exists and that the output directory is writable.
 @seealso
 - \code{\link[DBI]{dbConnect}} for database connections
 - \code{\link[duckdb]{duckdb}} for DuckDB database
 - \code{\link[arrow]{open_dataset}} for working with Arrow datasets
 - \code{\link[progressr]{with_progress}} for progress reporting
 - \code{\link[glue]{glue}} for string interpolation
 - \code{\link[jsonlite]{toJSON}} for JSON handling
 - \code{\link[readr]{read_csv}} for reading CSV files}
\usage{
txt_to_parquet_dataset(
  in_file,
  out_dir = "parquet_dataset",
  chunk_size = NULL,
  dbdir = ":memory:",
  partition_cols = NULL,
  return_arrow = FALSE,
  compression = "ZSTD",
  repartition = NULL,
  log_perf = TRUE,
  log_file = NULL
)
}
\description{
Convert a large text file (CSV/TSV) to a Parquet dataset with optional partitioning.
This function uses DuckDB for efficient chunked reading and writing.
 @param in_file Path to the input text file (CSV/TSV).
 @param out_dir Directory to save the Parquet dataset (default: "par
 quet_dataset").
 @param chunk_size Number of rows per chunk (default: auto-calculated for
 ~200 MB chunks).
 @param dbdir Path to DuckDB database file (default: in-memory).
 @param partition_cols Character vector of column names to partition by (default: NULL).
 @param return_arrow Logical, whether to return an Arrow dataset object (default: FALSE).
 @param compression Compression algorithm for Parquet files (default: "ZSTD").
 @param repartition Optional integer to repartition the dataset after writing (default: NULL).
 @param log_perf Logical, whether to log performance metrics (default: TRUE).
 @param log_file Optional path to a log file to save performance metrics (default: NULL).
 @return A list containing:
 \item{summary}{A data frame summarizing the partitioning (if any).}
 \item{dataset}{(Optional) An Arrow dataset object if return_arrow is TRUE.}
 \item{perf}{A list of performance metrics including time taken, rows processed, input/output sizes, etc.}
 @examples
 \dontrun{
 result <- txt_to_parquet_dataset(
     in_file = "large_data.csv",
     out_dir = "parquet_data",
     chunk_size = 1e6,
     partition_cols = c("year", "month"),
     return_arrow = TRUE,
     compression = "ZSTD"
     )
 print(result$summary)
 print(result$perf)
 if (!is.null(result$dataset)) {
     print(result$dataset)
 }
 }
 @import DBI duckdb progressr glue jsonlite readr
 @importFrom arrow open_dataset
 @importFrom utils write.csv
 @export
 @details
 This function reads a large text file (CSV/TSV) in chunks using DuckDB,
 writes the data to Parquet files with optional partitioning, and provides
 performance metrics. It can also return an Arrow dataset object for further
 analysis in R.
 The function automatically calculates an optimal chunk size to balance
 memory usage and performance, targeting approximately 200 MB per chunk.
 If partitioning columns are specified, the output Parquet files will be organized
 into subdirectories based on the unique values of those columns.
 Compression can be specified to reduce the size of the output files, with "ZSTD"
 being the default for a good balance of speed and compression ratio.
 Performance metrics include total time taken, rows processed, input and output
 file sizes, and rows per second.
 If a log file path is provided, the performance metrics will be saved in JSON
 format for easy parsing and analysis.
 Note that this function requires the DuckDB and Arrow packages to be installed
 and may require additional system dependencies for optimal performance.
 Ensure that the input file exists and that the output directory is writable.
 @seealso
 - \code{\link[DBI]{dbConnect}} for database connections
 - \code{\link[duckdb]{duckdb}} for DuckDB database
 - \code{\link[arrow]{open_dataset}} for working with Arrow datasets
 - \code{\link[progressr]{with_progress}} for progress reporting
 - \code{\link[glue]{glue}} for string interpolation
 - \code{\link[jsonlite]{toJSON}} for JSON handling
 - \code{\link[readr]{read_csv}} for reading CSV files
}
